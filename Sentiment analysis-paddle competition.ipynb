{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 情感分析\n",
    "这里用飞桨的高层API快速搭建模型实现情感分析比赛的结果的提交。具体的原理和分析请参考[『NLP打卡营』实践课5：文本情感分析](https://aistudio.baidu.com/aistudio/projectdetail/1968542?channelType=0&channel=0)。以下将分三部分：句子级情感分析（NLPCC14-SC,ChnSentiCorp）；目标级情感分析（SE-ABSA16_PHNS,SE-ABSA16_CAME）；以及观点抽取（COTE-BD，COTE-DP，COTE-MFW）。\n",
    "\n",
    "项目的使用非常简单，更改相应章节的data_name，并自己调整batch_size和epochs等以达到最佳的训练效果，并运行相应章节的所有代码即可得到对应数据集的预测结果。所有数据预测完成后，下载submission文件夹提交即可。\n",
    "\n",
    "**2021/6/18更新：添加了get_data_loader函数里的返回data_loader的shuffle选项（修复bug）；更改了观点抽取中保存文件的名称（修复bug）**\n",
    "\n",
    "**2016/6/20：更改了2,3章shuffle的错误拼写。**\n",
    "\n",
    "**2016/6/21：观点抽取中替换了英文字母编码时的特殊符号“##”，将\"\\[UNK\\]\"直接替换成了空字符（可以提高大概0.003的成绩）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: paddlenlp in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (2.0.3)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.9.0)\n",
      "Requirement already satisfied, skipping upgrade: multiprocess in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.70.11.1)\n",
      "Requirement already satisfied, skipping upgrade: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.1.1)\n",
      "Requirement already satisfied, skipping upgrade: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.42.1)\n",
      "Requirement already satisfied, skipping upgrade: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.2.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.7 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.20.3)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: dill>=0.3.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from multiprocess->paddlenlp) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.21.0)\n",
      "Requirement already satisfied, skipping upgrade: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.8.2)\n",
      "Requirement already satisfied, skipping upgrade: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.8.53)\n",
      "Requirement already satisfied, skipping upgrade: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.7.1.1)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: Pillow>=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.14.0)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (0.24.2)\n",
      "Requirement already satisfied, skipping upgrade: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.23)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (5.1.2)\n",
      "Requirement already satisfied, skipping upgrade: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.4)\n",
      "Requirement already satisfied, skipping upgrade: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (16.7.9)\n",
      "Requirement already satisfied, skipping upgrade: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.4.10)\n",
      "Requirement already satisfied, skipping upgrade: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: pycodestyle<2.7.0,>=2.6.0a1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.6.0)\n",
      "Requirement already satisfied, skipping upgrade: pyflakes<2.3.0,>=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: Jinja2>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.10.1)\n",
      "Requirement already satisfied, skipping upgrade: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (3.9.9)\n",
      "Requirement already satisfied, skipping upgrade: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (0.18.0)\n",
      "Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (1.25.6)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->pre-commit->visualdl->paddlenlp) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.5->Flask-Babel>=1.0.0->visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->pre-commit->visualdl->paddlenlp) (7.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade paddlenlp -i https://pypi.org/simple "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. 句子级情感分析\n",
    "句子级情感分析是针对输入的一段话，判断其感情倾向，一般为积极（1）或消极（0）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.0 载入模型和Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddlenlp\r\n",
    "from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.1 数据处理\n",
    "虽然一些数据集在PaddleNLP已存在，但是为了数据处理上的一致性，这里统一从上传的datasets中处理。对于PaddleNLP已存在的数据集，强烈建议直接用API调用，非常方便。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  datasets/ChnSentiCorp.zip\n",
      "  inflating: ChnSentiCorp/train.tsv  \n",
      "  inflating: __MACOSX/ChnSentiCorp/._train.tsv  \n",
      "  inflating: ChnSentiCorp/dev.tsv    \n",
      "  inflating: __MACOSX/ChnSentiCorp/._dev.tsv  \n",
      "  inflating: ChnSentiCorp/License.pdf  \n",
      "  inflating: __MACOSX/ChnSentiCorp/._License.pdf  \n",
      "  inflating: ChnSentiCorp/test.tsv   \n",
      "  inflating: __MACOSX/ChnSentiCorp/._test.tsv  \n",
      "  inflating: __MACOSX/._ChnSentiCorp  \n",
      "Archive:  datasets/NLPCC14-SC.zip\n",
      "  inflating: NLPCC14-SC/train.tsv    \n",
      "  inflating: __MACOSX/NLPCC14-SC/._train.tsv  \n",
      "  inflating: NLPCC14-SC/License.pdf  \n",
      "  inflating: __MACOSX/NLPCC14-SC/._License.pdf  \n",
      "  inflating: NLPCC14-SC/test.tsv     \n",
      "  inflating: __MACOSX/NLPCC14-SC/._test.tsv  \n",
      "  inflating: __MACOSX/._NLPCC14-SC   \n"
     ]
    }
   ],
   "source": [
    "# 解压数据\r\n",
    "!unzip -o datasets/ChnSentiCorp\r\n",
    "!unzip -o datasets/NLPCC14-SC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "数据内部结构解析：\n",
    "\n",
    "```\n",
    "ChnSentiCorp:\n",
    "\n",
    "train: \n",
    "label\t\ttext_a\n",
    "0\t\t房间太小。其他的都一般。。。。。。。。。\n",
    "1\t\t轻便，方便携带，性能也不错，能满足平时的工作需要，对出差人员来说非常不错\n",
    "\n",
    "dev:\n",
    "qid\t\tlabel\t\ttext_a\n",
    "0\t\t1\t\t這間酒店環境和服務態度亦算不錯,但房間空間太小~...\n",
    "\n",
    "test:\n",
    "qid\t\ttext_a\n",
    "0\t\t这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般\n",
    "...\t\t...\n",
    "\n",
    "\n",
    "NLPCC14-SC:\n",
    "\n",
    "train:\n",
    "label\t\ttext_a\n",
    "1\t\t请问这机不是有个遥控器的吗？\n",
    "0\t\t全是大道理啊\n",
    "\n",
    "test:\n",
    "qid\t\ttext_a\n",
    "0\t\t我终于找到同道中人啦～～～～从初中开始，我就...\n",
    "...\t\t...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "从上可以看出两个数据集可以定义一致的读取方式，但是NLPCC14-SC没有dev数据集，因此不再定义dev数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 得到数据集字典\r\n",
    "def open_func(file_path):\r\n",
    "    return [line.strip() for line in open(file_path, 'r', encoding='utf8').readlines()[1:] if len(line.strip().split('\\t')) >= 2]\r\n",
    "\r\n",
    "data_dict = {'chnsenticorp': {'test': open_func('ChnSentiCorp/test.tsv'),\r\n",
    "                              'dev': open_func('ChnSentiCorp/dev.tsv'),\r\n",
    "                              'train': open_func('ChnSentiCorp/train.tsv')},\r\n",
    "             'nlpcc14sc': {'test': open_func('NLPCC14-SC/test.tsv'),\r\n",
    "                           'train': open_func('NLPCC14-SC/train.tsv')}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.2 定义数据读取器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义数据集\r\n",
    "from paddle.io import Dataset, DataLoader\r\n",
    "from paddlenlp.data import Pad, Stack, Tuple\r\n",
    "import numpy as np\r\n",
    "label_list = [0, 1]\r\n",
    "\r\n",
    "# 注意，由于token type在此项任务中并没有起作用，因此这里不再考虑，让模型自行填充。\r\n",
    "class MyDataset(Dataset):\r\n",
    "    def __init__(self, data, tokenizer, max_len=512, for_test=False):\r\n",
    "        super().__init__()\r\n",
    "        self._data = data\r\n",
    "        self._tokenizer = tokenizer\r\n",
    "        self._max_len = max_len\r\n",
    "        self._for_test = for_test\r\n",
    "    \r\n",
    "    def __len__(self):\r\n",
    "        return len(self._data)\r\n",
    "    \r\n",
    "    def __getitem__(self, idx):\r\n",
    "        samples = self._data[idx].split('\\t')\r\n",
    "        label = samples[-2]\r\n",
    "        text = samples[-1]\r\n",
    "        label = int(label)\r\n",
    "        text = self._tokenizer.encode(text, max_seq_len=self._max_len)['input_ids']\r\n",
    "        if self._for_test:\r\n",
    "            return np.array(text, dtype='int64')\r\n",
    "        else:\r\n",
    "            return np.array(text, dtype='int64'), np.array(label, dtype='int64')\r\n",
    "\r\n",
    "def batchify_fn(for_test=False):\r\n",
    "    if for_test:\r\n",
    "        return lambda samples, fn=Pad(axis=0, pad_val=tokenizer.pad_token_id): np.row_stack([data for data in fn(samples)])\r\n",
    "    else:\r\n",
    "        return lambda samples, fn=Tuple(Pad(axis=0, pad_val=tokenizer.pad_token_id),\r\n",
    "                                        Stack()): [data for data in fn(samples)]\r\n",
    "\r\n",
    "\r\n",
    "def get_data_loader(data, tokenizer, batch_size=32, max_len=512, for_test=False):\r\n",
    "    dataset = MyDataset(data, tokenizer, max_len, for_test)\r\n",
    "    shuffle = True if not for_test else False\r\n",
    "    data_loader = DataLoader(dataset=dataset, batch_size=batch_size, collate_fn=batchify_fn(for_test), shuffle=shuffle)\r\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.3 模型搭建并进行训练\n",
    "模型非常简单，我们只需要调用对应的序列分类工具就行了。为了方便训练，直接用高层API Model完成训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-24 18:15:08,849] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.pdparams\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "[2021-06-24 18:15:19,253] [    INFO] - Found /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.vocab.txt\n"
     ]
    }
   ],
   "source": [
    "import paddle\r\n",
    "from paddle.static import InputSpec\r\n",
    "\r\n",
    "# 模型和分词\r\n",
    "model = SkepForSequenceClassification.from_pretrained('skep_ernie_1.0_large_ch', num_classes=2)\r\n",
    "tokenizer = SkepTokenizer.from_pretrained('skep_ernie_1.0_large_ch')\r\n",
    "\r\n",
    "# 参数设置\r\n",
    "data_name = 'nlpcc14sc'  # 更改此选项改变数据集\r\n",
    "\r\n",
    "## 训练相关\r\n",
    "epochs = 1\r\n",
    "learning_rate = 2e-5\r\n",
    "batch_size = 8\r\n",
    "max_len = 512\r\n",
    "\r\n",
    "## 数据相关\r\n",
    "train_dataloader = get_data_loader(data_dict[data_name]['train'], tokenizer, batch_size, max_len, for_test=False)\r\n",
    "if data_name == 'chnsenticorp':\r\n",
    "    dev_dataloader = get_data_loader(data_dict[data_name]['dev'], tokenizer, batch_size, max_len, for_test=False)\r\n",
    "else:\r\n",
    "    dev_dataloader = None\r\n",
    "\r\n",
    "input = InputSpec((-1, -1), dtype='int64', name='input')\r\n",
    "label = InputSpec((-1, 2), dtype='int64', name='label')\r\n",
    "model = paddle.Model(model, [input], [label])\r\n",
    "\r\n",
    "# 模型准备\r\n",
    "\r\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, parameters=model.parameters())\r\n",
    "model.prepare(optimizer, loss=paddle.nn.CrossEntropyLoss(), metrics=[paddle.metric.Accuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  return (isinstance(seq, collections.Sequence) and\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  200/1200 - loss: 0.0419 - acc: 0.8512 - 397ms/step\n",
      "step  400/1200 - loss: 0.0562 - acc: 0.8878 - 395ms/step\n",
      "step  600/1200 - loss: 0.1042 - acc: 0.8975 - 389ms/step\n",
      "step  800/1200 - loss: 0.3152 - acc: 0.9039 - 384ms/step\n",
      "step 1000/1200 - loss: 0.1925 - acc: 0.9101 - 386ms/step\n",
      "step 1200/1200 - loss: 0.2125 - acc: 0.9137 - 383ms/step\n",
      "save checkpoint at /home/aistudio/checkpoints/0\n",
      "Eval begin...\n",
      "step 150/150 - loss: 0.0691 - acc: 0.9433 - 133ms/step\n",
      "Eval samples: 1200\n",
      "save checkpoint at /home/aistudio/checkpoints/final\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\r\n",
    "model.fit(train_dataloader, dev_dataloader, batch_size, epochs, eval_freq=5, save_freq=5, save_dir='./checkpoints', log_freq=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.4 预测并保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-24 18:24:50,024] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.pdparams\n"
     ]
    }
   ],
   "source": [
    "# 导入预训练模型\r\n",
    "checkpoint_path = './checkpoints/final'  # 填写预训练模型的保存路径\r\n",
    "\r\n",
    "model = SkepForSequenceClassification.from_pretrained('skep_ernie_1.0_large_ch', num_classes=2)\r\n",
    "input = InputSpec((-1, -1), dtype='int64', name='input')\r\n",
    "model = paddle.Model(model, input)\r\n",
    "model.load(checkpoint_path)\r\n",
    "\r\n",
    "# 导入测试集\r\n",
    "test_dataloader = get_data_loader(data_dict[data_name]['test'], tokenizer, batch_size, max_len, for_test=True)\r\n",
    "# 预测保存\r\n",
    "\r\n",
    "save_file = {'chnsenticorp': './submission/ChnSentiCorp.tsv', 'nlpcc14sc': './submission/NLPCC14-SC.tsv'}\r\n",
    "predicts = []\r\n",
    "for batch in test_dataloader:\r\n",
    "    predict = model.predict_batch(batch)\r\n",
    "    predicts += predict[0].argmax(axis=-1).tolist()\r\n",
    "\r\n",
    "with open(save_file[data_name], 'w', encoding='utf8') as f:\r\n",
    "    f.write(\"index\\tprediction\\n\")\r\n",
    "    for idx, sample in enumerate(data_dict[data_name]['test']):\r\n",
    "        qid = sample.split('\\t')[0]\r\n",
    "        f.write(qid + '\\t' + str(predicts[idx]) + '\\n')\r\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. 目标级情感分析\n",
    "目标级情感分析将对整句的情感倾向扩充为对多个特定属性的情感倾向，本质上仍然是序列分类，但是针对同一个序列需要进行多次分类，针对不同的属性。这里的思路是将针对的属性也作为输入的一部分传入模型，并预测情感倾向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.0 载入模型和Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddlenlp\r\n",
    "from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.1 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  datasets/SE-ABSA16_CAME.zip\n",
      "  inflating: SE-ABSA16_CAME/train.tsv  \n",
      "  inflating: __MACOSX/SE-ABSA16_CAME/._train.tsv  \n",
      "  inflating: SE-ABSA16_CAME/License.pdf  \n",
      "  inflating: __MACOSX/SE-ABSA16_CAME/._License.pdf  \n",
      "  inflating: SE-ABSA16_CAME/test.tsv  \n",
      "  inflating: __MACOSX/SE-ABSA16_CAME/._test.tsv  \n",
      "  inflating: __MACOSX/._SE-ABSA16_CAME  \n",
      "Archive:  datasets/SE-ABSA16_PHNS.zip\n",
      "  inflating: SE-ABSA16_PHNS/train.tsv  \n",
      "  inflating: __MACOSX/SE-ABSA16_PHNS/._train.tsv  \n",
      "  inflating: SE-ABSA16_PHNS/License.pdf  \n",
      "  inflating: __MACOSX/SE-ABSA16_PHNS/._License.pdf  \n",
      "  inflating: SE-ABSA16_PHNS/test.tsv  \n",
      "  inflating: __MACOSX/SE-ABSA16_PHNS/._test.tsv  \n",
      "  inflating: __MACOSX/._SE-ABSA16_PHNS  \n"
     ]
    }
   ],
   "source": [
    "# 解压数据\r\n",
    "!unzip -o datasets/SE-ABSA16_CAME\r\n",
    "!unzip -o datasets/SE-ABSA16_PHNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "数据内部结构解析（两个数据集的结构相同）：\n",
    "```\n",
    "train:\n",
    "label\t\ttext_a\t\ttext_b\n",
    "1\t\tphone#design_features\t\t今天有幸拿到了港版白色iPhone 5真机，试玩了一下，说说感受吧：1. 真机尺寸宽度与4/4s保持一致没有变化...\n",
    "0\t\tsoftware#operation_performance\t\t苹果iPhone5新机到手 对比4S使用感受1，外观。一开始看发布会和网上照片，我和大多数人观点一样：变化不大，有点小失望。...\n",
    "\n",
    "test:\n",
    "qid\t\ttext_a\t\ttext_b\n",
    "0\t\tsoftware#usability\t\t刚刚入手8600，体会。刚刚从淘宝购买，1635元（包邮）。1、全新，...\n",
    "...\t\t...\t\t..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 得到数据集字典\r\n",
    "# 得到数据集字典\r\n",
    "def open_func(file_path):\r\n",
    "    return [line.strip() for line in open(file_path, 'r', encoding='utf8').readlines()[1:] if len(line.strip().split('\\t')) >= 2]\r\n",
    "\r\n",
    "data_dict = {'seabsa16phns': {'test': open_func('SE-ABSA16_PHNS/test.tsv'),\r\n",
    "                              'train': open_func('SE-ABSA16_PHNS/train.tsv')},\r\n",
    "             'seabsa16came': {'test': open_func('SE-ABSA16_CAME/test.tsv'),\r\n",
    "                              'train': open_func('SE-ABSA16_CAME/train.tsv')}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.2 定义数据读取器\n",
    "方法与1.2中相似，基本是完全粘贴复制过来即可。这里注意需要两个text，并且要考虑token_type_id了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义数据集\r\n",
    "from paddle.io import Dataset, DataLoader\r\n",
    "from paddlenlp.data import Pad, Stack, Tuple\r\n",
    "import numpy as np\r\n",
    "label_list = [0, 1]\r\n",
    "\r\n",
    "# 考虑token_type_id\r\n",
    "class MyDataset(Dataset):\r\n",
    "    def __init__(self, data, tokenizer, max_len=512, for_test=False):\r\n",
    "        super().__init__()\r\n",
    "        self._data = data\r\n",
    "        self._tokenizer = tokenizer\r\n",
    "        self._max_len = max_len\r\n",
    "        self._for_test = for_test\r\n",
    "    \r\n",
    "    def __len__(self):\r\n",
    "        return len(self._data)\r\n",
    "    \r\n",
    "    def __getitem__(self, idx):\r\n",
    "        samples = self._data[idx].split('\\t')\r\n",
    "        label = samples[-3]\r\n",
    "        text_b = samples[-1]\r\n",
    "        text_a = samples[-2]\r\n",
    "        label = int(label)\r\n",
    "        encoder_out = self._tokenizer.encode(text_a, text_b, max_seq_len=self._max_len)\r\n",
    "        text = encoder_out['input_ids']\r\n",
    "        token_type = encoder_out['token_type_ids']\r\n",
    "        if self._for_test:\r\n",
    "            return np.array(text, dtype='int64'), np.array(token_type, dtype='int64')\r\n",
    "        else:\r\n",
    "            return np.array(text, dtype='int64'), np.array(token_type, dtype='int64'), np.array(label, dtype='int64')\r\n",
    "\r\n",
    "def batchify_fn(for_test=False):\r\n",
    "    if for_test:\r\n",
    "        return lambda samples, fn=Tuple(Pad(axis=0, pad_val=tokenizer.pad_token_id),\r\n",
    "                                        Pad(axis=0, pad_val=tokenizer.pad_token_type_id)): [data for data in fn(samples)]\r\n",
    "    else:\r\n",
    "        return lambda samples, fn=Tuple(Pad(axis=0, pad_val=tokenizer.pad_token_id),\r\n",
    "                                        Pad(axis=0, pad_val=tokenizer.pad_token_type_id),\r\n",
    "                                        Stack()): [data for data in fn(samples)]\r\n",
    "\r\n",
    "\r\n",
    "def get_data_loader(data, tokenizer, batch_size=32, max_len=512, for_test=False):\r\n",
    "    dataset = MyDataset(data, tokenizer, max_len, for_test)\r\n",
    "    shuffle = True if not for_test else False\r\n",
    "    data_loader = DataLoader(dataset=dataset, batch_size=batch_size, collate_fn=batchify_fn(for_test), shuffle=shuffle)\r\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.3 模型搭建并进行训练\n",
    "把1.3的复制粘贴过来，注意该数据集名称，并加上一个token_type_id的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-24 18:25:41,729] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.pdparams\n",
      "[2021-06-24 18:25:46,441] [    INFO] - Found /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.vocab.txt\n"
     ]
    }
   ],
   "source": [
    "import paddle\r\n",
    "from paddle.static import InputSpec\r\n",
    "\r\n",
    "# 模型和分词\r\n",
    "model = SkepForSequenceClassification.from_pretrained('skep_ernie_1.0_large_ch', num_classes=2)\r\n",
    "tokenizer = SkepTokenizer.from_pretrained('skep_ernie_1.0_large_ch')\r\n",
    "\r\n",
    "# 参数设置\r\n",
    "data_name = 'seabsa16came'  # 更改此选项改变数据集\r\n",
    "\r\n",
    "## 训练相关\r\n",
    "epochs = 1\r\n",
    "learning_rate = 2e-5\r\n",
    "batch_size = 8\r\n",
    "max_len = 512\r\n",
    "\r\n",
    "## 数据相关\r\n",
    "train_dataloader = get_data_loader(data_dict[data_name]['train'], tokenizer, batch_size, max_len, for_test=False)\r\n",
    "\r\n",
    "input = InputSpec((-1, -1), dtype='int64', name='input')\r\n",
    "token_type = InputSpec((-1, -1), dtype='int64', name='token_type')\r\n",
    "label = InputSpec((-1, 2), dtype='int64', name='label')\r\n",
    "model = paddle.Model(model, [input, token_type], [label])\r\n",
    "\r\n",
    "# 模型准备\r\n",
    "\r\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, parameters=model.parameters())\r\n",
    "model.prepare(optimizer, loss=paddle.nn.CrossEntropyLoss(), metrics=[paddle.metric.Accuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/1\n",
      "step 165/165 - loss: 0.5355 - acc: 0.6363 - 817ms/step\n",
      "save checkpoint at /home/aistudio/checkpoints/0\n",
      "save checkpoint at /home/aistudio/checkpoints/final\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\r\n",
    "model.fit(train_dataloader, batch_size=batch_size, epochs=epochs, save_freq=5, save_dir='./checkpoints', log_freq=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.4 预测并保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-24 18:29:29,712] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.pdparams\n"
     ]
    }
   ],
   "source": [
    "# 导入预训练模型\r\n",
    "checkpoint_path = './checkpoints/final'  # 填写预训练模型的保存路径\r\n",
    "\r\n",
    "model = SkepForSequenceClassification.from_pretrained('skep_ernie_1.0_large_ch', num_classes=2)\r\n",
    "input = InputSpec((-1, -1), dtype='int64', name='input')\r\n",
    "token_type = InputSpec((-1, -1), dtype='int64', name='token_type')\r\n",
    "model = paddle.Model(model, [input, token_type])\r\n",
    "model.load(checkpoint_path)\r\n",
    "\r\n",
    "# 导入测试集\r\n",
    "test_dataloader = get_data_loader(data_dict[data_name]['test'], tokenizer, batch_size, max_len, for_test=True)\r\n",
    "# 预测保存\r\n",
    "\r\n",
    "save_file = {'seabsa16phns': './submission/SE-ABSA16_PHNS.tsv', 'seabsa16came': './submission/SE-ABSA16_CAME.tsv'}\r\n",
    "predicts = []\r\n",
    "for batch in test_dataloader:\r\n",
    "    predict = model.predict_batch(batch)\r\n",
    "    predicts += predict[0].argmax(axis=-1).tolist()\r\n",
    "\r\n",
    "with open(save_file[data_name], 'w', encoding='utf8') as f:\r\n",
    "    f.write(\"index\\tprediction\\n\")\r\n",
    "    for idx, sample in enumerate(data_dict[data_name]['test']):\r\n",
    "        qid = sample.split('\\t')[0]\r\n",
    "        f.write(qid + '\\t' + str(predicts[idx]) + '\\n')\r\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. 观点抽取\n",
    "### 3.0 载入模型和Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddlenlp\r\n",
    "from paddlenlp.transformers import SkepForTokenClassification, SkepTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.1 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  datasets/COTE-BD.zip\n",
      "  inflating: COTE-BD/train.tsv       \n",
      "  inflating: __MACOSX/COTE-BD/._train.tsv  \n",
      "  inflating: COTE-BD/License.pdf     \n",
      "  inflating: __MACOSX/COTE-BD/._License.pdf  \n",
      "  inflating: COTE-BD/test.tsv        \n",
      "  inflating: __MACOSX/COTE-BD/._test.tsv  \n",
      "  inflating: __MACOSX/._COTE-BD      \n",
      "Archive:  datasets/COTE-DP.zip\n",
      "  inflating: COTE-DP/train.tsv       \n",
      "  inflating: __MACOSX/COTE-DP/._train.tsv  \n",
      "  inflating: COTE-DP/License.pdf     \n",
      "  inflating: __MACOSX/COTE-DP/._License.pdf  \n",
      "  inflating: COTE-DP/test.tsv        \n",
      "  inflating: __MACOSX/COTE-DP/._test.tsv  \n",
      "  inflating: __MACOSX/._COTE-DP      \n",
      "Archive:  datasets/COTE-MFW.zip\n",
      "  inflating: COTE-MFW/train.tsv      \n",
      "  inflating: __MACOSX/COTE-MFW/._train.tsv  \n",
      "  inflating: COTE-MFW/License.pdf    \n",
      "  inflating: __MACOSX/COTE-MFW/._License.pdf  \n",
      "  inflating: COTE-MFW/test.tsv       \n",
      "  inflating: __MACOSX/COTE-MFW/._test.tsv  \n",
      "  inflating: __MACOSX/._COTE-MFW     \n"
     ]
    }
   ],
   "source": [
    "# 解压数据\r\n",
    "!unzip -o datasets/COTE-BD\r\n",
    "!unzip -o datasets/COTE-DP\r\n",
    "!unzip -o datasets/COTE-MFW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "数据内部结构解析（三个数据集的结构相同）：\n",
    "```\n",
    "train:\n",
    "label\t\ttext_a\n",
    "鸟人\t\t《鸟人》一书以鸟博士的遭遇作为主线，主要写了鸟博士从校园出来后的种种荒诞经历。\n",
    "...\t\t...\n",
    "test:\n",
    "qid\t\ttext_a\n",
    "0\t\t毕棚沟的风景早有所闻，尤其以秋季的风景最美，但是这次去晚了，红叶全掉完了，黄叶也看不到了，下了雪只...\n",
    "...\t\t..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 得到数据集字典\r\n",
    "def open_func(file_path):\r\n",
    "    return [line.strip() for line in open(file_path, 'r', encoding='utf8').readlines()[1:] if len(line.strip().split('\\t')) >= 2]\r\n",
    "\r\n",
    "data_dict = {'cotebd': {'test': open_func('COTE-BD/test.tsv'),\r\n",
    "                        'train': open_func('COTE-BD/train.tsv')},\r\n",
    "             'cotedp': {'test': open_func('COTE-DP/test.tsv'),\r\n",
    "                        'train': open_func('COTE-DP/train.tsv')},\r\n",
    "             'cotemfw': {'test': open_func('COTE-MFW/test.tsv'),\r\n",
    "                        'train': open_func('COTE-MFW/train.tsv')}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.2 定义数据读取器\n",
    "思路类似，需要注意的是这一次是Tokens级的分类。在数据读取器中，将label写成BIO的形式，每一个token都对应一个label。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义数据集\r\n",
    "from paddle.io import Dataset, DataLoader\r\n",
    "from paddlenlp.data import Pad, Stack, Tuple\r\n",
    "import numpy as np\r\n",
    "label_list = {'B': 0, 'I': 1, 'O': 2}\r\n",
    "index2label = {0: 'B', 1: 'I', 2: 'O'}\r\n",
    "\r\n",
    "# 考虑token_type_id\r\n",
    "class MyDataset(Dataset):\r\n",
    "    def __init__(self, data, tokenizer, max_len=512, for_test=False):\r\n",
    "        super().__init__()\r\n",
    "        self._data = data\r\n",
    "        self._tokenizer = tokenizer\r\n",
    "        self._max_len = max_len\r\n",
    "        self._for_test = for_test\r\n",
    "    \r\n",
    "    def __len__(self):\r\n",
    "        return len(self._data)\r\n",
    "    \r\n",
    "    def __getitem__(self, idx):\r\n",
    "        samples = self._data[idx].split('\\t')\r\n",
    "        label = samples[-2]\r\n",
    "        text = samples[-1]\r\n",
    "        if self._for_test:\r\n",
    "            origin_enc = self._tokenizer.encode(text, max_seq_len=self._max_len)['input_ids']\r\n",
    "            return np.array(origin_enc, dtype='int64')\r\n",
    "        else:\r\n",
    "            \r\n",
    "            # 由于并不是每个字都是一个token，这里采用一种简单的处理方法，先编码label，再编码text中除了label以外的词，最后合到一起\r\n",
    "            texts = text.split(label)\r\n",
    "            label_enc = self._tokenizer.encode(label)['input_ids']\r\n",
    "            cls_enc = label_enc[0]\r\n",
    "            sep_enc = label_enc[-1]\r\n",
    "            label_enc = label_enc[1:-1]\r\n",
    "            \r\n",
    "            # 合并\r\n",
    "            origin_enc = []\r\n",
    "            label_ids = []\r\n",
    "            for index, text in enumerate(texts):\r\n",
    "                text_enc = self._tokenizer.encode(text)['input_ids']\r\n",
    "                text_enc = text_enc[1:-1]\r\n",
    "                origin_enc += text_enc\r\n",
    "                label_ids += [label_list['O']] * len(text_enc)\r\n",
    "                if index != len(texts) - 1:\r\n",
    "                    origin_enc += label_enc\r\n",
    "                    label_ids += [label_list['B']] + [label_list['I']] * (len(label_enc) - 1)\r\n",
    "\r\n",
    "            origin_enc = [cls_enc] + origin_enc + [sep_enc]\r\n",
    "            label_ids = [label_list['O']] + label_ids + [label_list['O']]\r\n",
    "            \r\n",
    "            # 截断\r\n",
    "            if len(origin_enc) > self._max_len:\r\n",
    "                origin_enc = origin_enc[:self._max_len-1] + origin_enc[-1:]\r\n",
    "                label_ids = label_ids[:self._max_len-1] + label_ids[-1:]\r\n",
    "            return np.array(origin_enc, dtype='int64'), np.array(label_ids, dtype='int64')\r\n",
    "\r\n",
    "\r\n",
    "def batchify_fn(for_test=False):\r\n",
    "    if for_test:\r\n",
    "        return lambda samples, fn=Pad(axis=0, pad_val=tokenizer.pad_token_id): np.row_stack([data for data in fn(samples)])\r\n",
    "    else:\r\n",
    "        return lambda samples, fn=Tuple(Pad(axis=0, pad_val=tokenizer.pad_token_id),\r\n",
    "                                        Pad(axis=0, pad_val=label_list['O'])): [data for data in fn(samples)]\r\n",
    "\r\n",
    "\r\n",
    "def get_data_loader(data, tokenizer, batch_size=32, max_len=512, for_test=False):\r\n",
    "    dataset = MyDataset(data, tokenizer, max_len, for_test)\r\n",
    "    shuffle = True if not for_test else False\r\n",
    "    data_loader = DataLoader(dataset=dataset, batch_size=batch_size, collate_fn=batchify_fn(for_test), shuffle=shuffle)\r\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.3 模型搭建并进行训练\n",
    "与之前不同的是模型换成了Token分类。由于Accuracy不再适用于Token分类，我们用Perplexity来大致衡量预测的准确度（接近1为最佳）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-24 19:06:02,074] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.pdparams\n",
      "[2021-06-24 19:06:06,867] [    INFO] - Found /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.vocab.txt\n"
     ]
    }
   ],
   "source": [
    "import paddle\r\n",
    "from paddle.static import InputSpec\r\n",
    "from paddlenlp.metrics import Perplexity\r\n",
    "\r\n",
    "# 模型和分词\r\n",
    "model = SkepForTokenClassification.from_pretrained('skep_ernie_1.0_large_ch', num_classes=3)\r\n",
    "tokenizer = SkepTokenizer.from_pretrained('skep_ernie_1.0_large_ch')\r\n",
    "\r\n",
    "# 参数设置\r\n",
    "data_name = 'cotemfw'  # 更改此选项改变数据集\r\n",
    "\r\n",
    "## 训练相关\r\n",
    "epochs = 1\r\n",
    "learning_rate = 2e-5\r\n",
    "batch_size = 8\r\n",
    "max_len = 512\r\n",
    "\r\n",
    "## 数据相关\r\n",
    "train_dataloader = get_data_loader(data_dict[data_name]['train'], tokenizer, batch_size, max_len, for_test=False)\r\n",
    "\r\n",
    "input = InputSpec((-1, -1), dtype='int64', name='input')\r\n",
    "label = InputSpec((-1, -1, 3), dtype='int64', name='label')\r\n",
    "model = paddle.Model(model, [input], [label])\r\n",
    "\r\n",
    "# 模型准备\r\n",
    "\r\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, parameters=model.parameters())\r\n",
    "model.prepare(optimizer, loss=paddle.nn.CrossEntropyLoss(), metrics=[Perplexity()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/1\n",
      "step  200/5157 - loss: 0.0316 - Perplexity: 1.0668 - 198ms/step\n",
      "step  400/5157 - loss: 0.0146 - Perplexity: 1.0472 - 199ms/step\n",
      "step  600/5157 - loss: 0.0313 - Perplexity: 1.0397 - 201ms/step\n",
      "step  800/5157 - loss: 0.0420 - Perplexity: 1.0356 - 200ms/step\n",
      "step 1000/5157 - loss: 0.0156 - Perplexity: 1.0329 - 201ms/step\n",
      "step 1200/5157 - loss: 0.0060 - Perplexity: 1.0316 - 199ms/step\n",
      "step 1400/5157 - loss: 0.0166 - Perplexity: 1.0306 - 198ms/step\n",
      "step 1600/5157 - loss: 0.0062 - Perplexity: 1.0294 - 199ms/step\n",
      "step 1800/5157 - loss: 0.0171 - Perplexity: 1.0283 - 199ms/step\n",
      "step 2000/5157 - loss: 0.0307 - Perplexity: 1.0278 - 199ms/step\n",
      "step 2200/5157 - loss: 0.0112 - Perplexity: 1.0273 - 198ms/step\n",
      "step 2400/5157 - loss: 0.0166 - Perplexity: 1.0269 - 197ms/step\n",
      "step 2600/5157 - loss: 0.0558 - Perplexity: 1.0265 - 197ms/step\n",
      "step 2800/5157 - loss: 0.0303 - Perplexity: 1.0260 - 197ms/step\n",
      "step 3000/5157 - loss: 0.0079 - Perplexity: 1.0257 - 197ms/step\n",
      "step 3200/5157 - loss: 0.0084 - Perplexity: 1.0253 - 197ms/step\n",
      "step 3400/5157 - loss: 0.0161 - Perplexity: 1.0251 - 197ms/step\n",
      "step 3600/5157 - loss: 0.0030 - Perplexity: 1.0249 - 197ms/step\n",
      "step 3800/5157 - loss: 0.0073 - Perplexity: 1.0245 - 197ms/step\n",
      "step 4000/5157 - loss: 0.0612 - Perplexity: 1.0243 - 197ms/step\n",
      "step 4200/5157 - loss: 0.0211 - Perplexity: 1.0242 - 197ms/step\n",
      "step 4400/5157 - loss: 0.0134 - Perplexity: 1.0240 - 196ms/step\n",
      "step 4600/5157 - loss: 0.0425 - Perplexity: 1.0238 - 196ms/step\n",
      "step 4800/5157 - loss: 0.0222 - Perplexity: 1.0235 - 196ms/step\n",
      "step 5000/5157 - loss: 0.0335 - Perplexity: 1.0234 - 196ms/step\n",
      "step 5157/5157 - loss: 0.0582 - Perplexity: 1.0233 - 196ms/step\n",
      "save checkpoint at /home/aistudio/checkpoints/0\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\r\n",
    "model.fit(train_dataloader, batch_size=batch_size, epochs=epochs, save_freq=5, save_dir='./checkpoints', log_freq=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.4 预测并保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\r\n",
    "\r\n",
    "# 导入预训练模型\r\n",
    "checkpoint_path = './checkpoints/final'  # 填写预训练模型的保存路径\r\n",
    "\r\n",
    "model = SkepForTokenClassification.from_pretrained('skep_ernie_1.0_large_ch', num_classes=3)\r\n",
    "input = InputSpec((-1, -1), dtype='int64', name='input')\r\n",
    "model = paddle.Model(model, [input])\r\n",
    "model.load(checkpoint_path)\r\n",
    "\r\n",
    "# 导入测试集\r\n",
    "test_dataloader = get_data_loader(data_dict[data_name]['test'], tokenizer, batch_size, max_len, for_test=True)\r\n",
    "# 预测保存\r\n",
    "\r\n",
    "save_file = {'cotebd': './submission/COTE_BD.tsv', 'cotedp': './submission/COTE_DP.tsv', 'cotemfw': './submission/COTE_MFW.tsv'}\r\n",
    "predicts = []\r\n",
    "input_ids = []\r\n",
    "for batch in test_dataloader:\r\n",
    "    predict = model.predict_batch(batch)\r\n",
    "    predicts += predict[0].argmax(axis=-1).tolist()\r\n",
    "    input_ids += batch.numpy().tolist()\r\n",
    "\r\n",
    "# 先找到B所在的位置，即标号为0的位置，然后顺着该位置一直找到所有的I，即标号为1，即为所得。\r\n",
    "def find_entity(prediction, input_ids):\r\n",
    "    entity = []\r\n",
    "    entity_ids = []\r\n",
    "    for index, idx in enumerate(prediction):\r\n",
    "        if idx == label_list['B']:\r\n",
    "            entity_ids = [input_ids[index]]\r\n",
    "        elif idx == label_list['I']:\r\n",
    "            if entity_ids:\r\n",
    "                entity_ids.append(input_ids[index])\r\n",
    "        elif idx == label_list['O']:\r\n",
    "            if entity_ids:\r\n",
    "                entity.append(''.join(tokenizer.convert_ids_to_tokens(entity_ids)))\r\n",
    "                entity_ids = []\r\n",
    "    return entity\r\n",
    "\r\n",
    "with open(save_file[data_name], 'w', encoding='utf8') as f:\r\n",
    "    f.write(\"index\\tprediction\\n\")\r\n",
    "    for idx, sample in enumerate(data_dict[data_name]['test']):\r\n",
    "        qid = sample.split('\\t')[0]\r\n",
    "        entity = find_entity(predicts[idx], input_ids[idx])\r\n",
    "        entity = list(set(entity))  # 去重\r\n",
    "        entity = [re.sub('##', '', e) for e in entity]  # 去除英文编码时的特殊符号\r\n",
    "        entity = [re.sub('[UNK]', '', e) for e in entity]  # 去除未知符号\r\n",
    "        f.write(qid + '\\t' + '\\x01'.join(entity) + '\\n')\r\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
